{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct the re-filtering by reloading the file and excluding the correct columns only\n",
    "df = pd.read_csv('data-files/transcript_data_w_lies_and_LDAtopics.csv').drop(columns=['Unnamed: 0', 'YouTube URL', 'Chunk Number', 'Video ID', \n",
    "                                                   'Speech ID', 'Speaker', 'Middle Class / American Dream', \n",
    "                                                   'National Security / Immigration', 'Appreciation', \n",
    "                                                   'Lives at Stake', 'Economic Growth / Job Creation'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df = pd.read_csv('data-files/individual_sentiment_analysis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the max sentiment for each row and assign the respective label ('neg', 'neu', 'pos')\n",
    "sentiment_df['sentiment'] = sentiment_df[['neg', 'neu', 'pos']].idxmax(axis=1)\n",
    "\n",
    "# Select only 'Chunk Filename' and 'sentiment' columns for merging\n",
    "sentiment_column = sentiment_df[['Chunk Filename', 'sentiment']]\n",
    "\n",
    "# Merge this sentiment column with the previously filtered DataFrame on 'Chunk Filename'\n",
    "df_with_sentiment = df.merge(sentiment_column, on='Chunk Filename', how='left')\n",
    "\n",
    "# Move the 'lie' column to the end\n",
    "df_with_sentiment = df_with_sentiment[[col for col in df_with_sentiment.columns if col != 'lie'] + ['lie']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the file paths in the 'Chunk Filename' column by replacing the old path with 'audio-files/'\n",
    "def update_file_paths(df):\n",
    "    df['Chunk Filename'] = df['Chunk Filename'].apply(\n",
    "        lambda x: x.replace(\"/Users/milanvaghani/Desktop/Unstructed Machine Learning/\", \"audio-files/\")\n",
    "        if x.startswith(\"/Users/milanvaghani/Desktop/Unstructed Machine Learning/VP\") else x\n",
    "    )\n",
    "    return df\n",
    "# Re-apply the process to filter out rows where the audio file does not exist\n",
    "def file_exists(row):\n",
    "    return os.path.isfile(row['Chunk Filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_sentiment = update_file_paths(df_with_sentiment)\n",
    "# Filter out rows where the file does not exist\n",
    "df_with_sentiment = df_with_sentiment[df_with_sentiment.apply(file_exists, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_file_path = 'data-files/combined.csv'\n",
    "df_with_sentiment.to_csv(combined_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6638 - loss: 3.2848 - val_accuracy: 0.3143 - val_loss: 0.8141\n",
      "Epoch 2/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3709 - loss: 1.9309 - val_accuracy: 0.2381 - val_loss: 1.2187\n",
      "Epoch 3/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2742 - loss: 1.7156 - val_accuracy: 0.3714 - val_loss: 1.1008\n",
      "Epoch 4/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3591 - loss: 1.8489 - val_accuracy: 0.4095 - val_loss: 1.0793\n",
      "Epoch 5/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3591 - loss: 1.6518 - val_accuracy: 0.4190 - val_loss: 0.9398\n",
      "Epoch 6/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4880 - loss: 1.4692 - val_accuracy: 0.4952 - val_loss: 0.9131\n",
      "Epoch 7/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4439 - loss: 1.7980 - val_accuracy: 0.4381 - val_loss: 1.0261\n",
      "Epoch 8/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3909 - loss: 1.6749 - val_accuracy: 0.4762 - val_loss: 0.9206\n",
      "Epoch 9/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5318 - loss: 1.5999 - val_accuracy: 0.4762 - val_loss: 0.9535\n",
      "Epoch 10/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4289 - loss: 1.7319 - val_accuracy: 0.4952 - val_loss: 0.9504\n",
      "Epoch 11/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4920 - loss: 1.4621 - val_accuracy: 0.4952 - val_loss: 0.9634\n",
      "Epoch 12/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4665 - loss: 1.8106 - val_accuracy: 0.5333 - val_loss: 0.8439\n",
      "Epoch 13/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5096 - loss: 1.6486 - val_accuracy: 0.4857 - val_loss: 0.9842\n",
      "Epoch 14/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4892 - loss: 1.4298 - val_accuracy: 0.5619 - val_loss: 0.8215\n",
      "Epoch 15/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5329 - loss: 1.5637 - val_accuracy: 0.5143 - val_loss: 0.8984\n",
      "Epoch 16/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4778 - loss: 1.4043 - val_accuracy: 0.4667 - val_loss: 1.0238\n",
      "Epoch 17/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4919 - loss: 1.3343 - val_accuracy: 0.5238 - val_loss: 0.8275\n",
      "Epoch 18/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5621 - loss: 1.6915 - val_accuracy: 0.5333 - val_loss: 0.8586\n",
      "Epoch 19/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5505 - loss: 1.3905 - val_accuracy: 0.5524 - val_loss: 0.8416\n",
      "Epoch 20/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5315 - loss: 1.3876 - val_accuracy: 0.5619 - val_loss: 0.8703\n",
      "Epoch 21/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5722 - loss: 1.5227 - val_accuracy: 0.5143 - val_loss: 0.9436\n",
      "Epoch 22/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5279 - loss: 1.2311 - val_accuracy: 0.6000 - val_loss: 0.7536\n",
      "Epoch 23/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5979 - loss: 1.1756 - val_accuracy: 0.6476 - val_loss: 0.6490\n",
      "Epoch 24/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6269 - loss: 1.2896 - val_accuracy: 0.6095 - val_loss: 0.7764\n",
      "Epoch 25/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5509 - loss: 1.4238 - val_accuracy: 0.6095 - val_loss: 0.8100\n",
      "Epoch 26/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5458 - loss: 1.2135 - val_accuracy: 0.6000 - val_loss: 0.7396\n",
      "Epoch 27/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6078 - loss: 1.3802 - val_accuracy: 0.5810 - val_loss: 0.7832\n",
      "Epoch 28/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6140 - loss: 1.0767 - val_accuracy: 0.6190 - val_loss: 0.7245\n",
      "Epoch 29/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6004 - loss: 1.2244 - val_accuracy: 0.6857 - val_loss: 0.6647\n",
      "Epoch 30/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6497 - loss: 1.3880 - val_accuracy: 0.6095 - val_loss: 0.7680\n",
      "Epoch 31/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6116 - loss: 1.3475 - val_accuracy: 0.6381 - val_loss: 0.6550\n",
      "Epoch 32/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6736 - loss: 1.0175 - val_accuracy: 0.6381 - val_loss: 0.6306\n",
      "Epoch 33/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6560 - loss: 1.2445 - val_accuracy: 0.6286 - val_loss: 0.7355\n",
      "Epoch 34/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5753 - loss: 1.0852 - val_accuracy: 0.6381 - val_loss: 0.6513\n",
      "Epoch 35/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6949 - loss: 1.0539 - val_accuracy: 0.6667 - val_loss: 0.6472\n",
      "Epoch 36/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6493 - loss: 1.1795 - val_accuracy: 0.6476 - val_loss: 0.6710\n",
      "Epoch 37/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6391 - loss: 1.0217 - val_accuracy: 0.6476 - val_loss: 0.7236\n",
      "Epoch 38/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6591 - loss: 1.1634 - val_accuracy: 0.6762 - val_loss: 0.6445\n",
      "Epoch 39/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6781 - loss: 1.0141 - val_accuracy: 0.6667 - val_loss: 0.7316\n",
      "Epoch 40/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7235 - loss: 0.9721 - val_accuracy: 0.6667 - val_loss: 0.7589\n",
      "Epoch 41/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6419 - loss: 0.9673 - val_accuracy: 0.6952 - val_loss: 0.6822\n",
      "Epoch 42/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7108 - loss: 0.9829 - val_accuracy: 0.6667 - val_loss: 0.6981\n",
      "Epoch 43/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6563 - loss: 1.1835 - val_accuracy: 0.7048 - val_loss: 0.6634\n",
      "Epoch 44/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6692 - loss: 0.9840 - val_accuracy: 0.7048 - val_loss: 0.6139\n",
      "Epoch 45/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7630 - loss: 0.8648 - val_accuracy: 0.6381 - val_loss: 0.8712\n",
      "Epoch 46/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6607 - loss: 0.9716 - val_accuracy: 0.6571 - val_loss: 0.7676\n",
      "Epoch 47/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6987 - loss: 0.9417 - val_accuracy: 0.7619 - val_loss: 0.5827\n",
      "Epoch 48/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8009 - loss: 0.7591 - val_accuracy: 0.7524 - val_loss: 0.5873\n",
      "Epoch 49/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7672 - loss: 0.8598 - val_accuracy: 0.6762 - val_loss: 0.7004\n",
      "Epoch 50/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7566 - loss: 0.8713 - val_accuracy: 0.7619 - val_loss: 0.6200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      0.78      0.86        96\n",
      "        True       0.19      0.56      0.29         9\n",
      "\n",
      "    accuracy                           0.76       105\n",
      "   macro avg       0.57      0.67      0.57       105\n",
      "weighted avg       0.88      0.76      0.81       105\n",
      "\n",
      "Confusion Matrix:\n",
      "[[75 21]\n",
      " [ 4  5]]\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Predictions saved to 'test_predictions_combined.csv'.\n"
     ]
    }
   ],
   "source": [
    "def extract_audio_features(file_name):\n",
    "    try:\n",
    "        audio_data, sample_rate = librosa.load(file_name, res_type='kaiser_fast')\n",
    "        mfccs = librosa.feature.mfcc(y=audio_data, sr=sample_rate, n_mfcc=40)\n",
    "        mfccs_scaled = np.mean(mfccs.T, axis=0)\n",
    "        mfccs_scaled_std = np.std(mfccs.T, axis=0)\n",
    "        features = np.hstack((mfccs_scaled, mfccs_scaled_std))\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"Error encountered while parsing file: {file_name}. Exception: {e}\")\n",
    "        return np.zeros(80)\n",
    "\n",
    "def main():\n",
    "    # Load the combined data\n",
    "    combined_df = pd.read_csv('data-files/combined.csv')\n",
    "\n",
    "    # Extract features and labels\n",
    "    audio_features = []\n",
    "    labels = []\n",
    "\n",
    "    for index, row in combined_df.iterrows():\n",
    "        audio_path = row['Chunk Filename']\n",
    "        label = row['lie']\n",
    "        \n",
    "        # Extract audio features\n",
    "        audio_data = extract_audio_features(audio_path)\n",
    "        \n",
    "        # Collect other features and convert to float\n",
    "        other_data = pd.to_numeric(row.drop(['Chunk Filename', 'Transcript', 'lie']), errors='coerce').values.astype(float)\n",
    "        \n",
    "        # Combine features and append\n",
    "        combined_data = np.hstack((audio_data, other_data))\n",
    "        audio_features.append(combined_data)\n",
    "        labels.append(label)\n",
    "\n",
    "    # Convert lists to arrays\n",
    "    X = np.array(audio_features, dtype=float)\n",
    "    y = np.array([int(label) for label in labels])\n",
    "\n",
    "    # Handle any NaNs in case of remaining issues\n",
    "    X = np.nan_to_num(X)\n",
    "\n",
    "    # Split the dataset\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "\n",
    "    # Build the neural network model with class weight adjustment\n",
    "    model = Sequential([\n",
    "        Dense(256, input_shape=(X_train.shape[1],), activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    # Compile with class weights to emphasize \"True\" class\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # Set class weights\n",
    "    class_weights = {0: 1, 1: 30} \n",
    "\n",
    "    # Train the model with class weights\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_val, y_val),\n",
    "        class_weight=class_weights,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred_prob = model.predict(X_val).flatten()\n",
    "    y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_val, y_pred, target_names=['False', 'True']))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_val, y_pred))\n",
    "\n",
    "    # Predict on test data\n",
    "    test_features = []\n",
    "    for index, row in combined_df.iterrows():\n",
    "        audio_path = row['Chunk Filename']\n",
    "        audio_data = extract_audio_features(audio_path)\n",
    "        other_data = pd.to_numeric(row.drop(['Chunk Filename', 'Transcript', 'lie']), errors='coerce').values.astype(float)\n",
    "        combined_data = np.hstack((audio_data, other_data))\n",
    "        test_features.append(combined_data)\n",
    "\n",
    "    X_test = np.array(test_features, dtype=float)\n",
    "    X_test = np.nan_to_num(X_test)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    test_predictions_prob = model.predict(X_test).flatten()\n",
    "    test_predictions = (test_predictions_prob > 0.5).astype(int)\n",
    "\n",
    "    # Map numeric predictions back to boolean values\n",
    "    test_labels = [bool(pred) for pred in test_predictions]\n",
    "\n",
    "    # Save predictions\n",
    "    combined_df['predicted_label'] = test_labels\n",
    "    combined_df['probability_true'] = test_predictions_prob\n",
    "    combined_df.to_csv('test_predictions_combined.csv', index=False)\n",
    "    print(\"Predictions saved to 'test_predictions_combined.csv'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
